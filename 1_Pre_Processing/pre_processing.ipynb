{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating datasets\n",
    "from sklearn.datasets import make_classification\n",
    "X,y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Techniques\n",
    "1. Data Cleaning <br>\n",
    "<t>    1.1 Missing Values Treatment <br>\n",
    "<t>    1.2 Noise Smoothening <br>\n",
    "<t>    1.3 Outlier Treatment \n",
    "2. Data Integration\n",
    "3. Data Transformation\n",
    "    3.1 Normalization\n",
    "    3.2 Noise removal- Binning\n",
    "4. Data Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Null Values\n",
    "1. Null Value Threshold for a column\n",
    "2. Null Value treatment for remaining Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/22 01:27:23 WARN Utils: Your hostname, pop-os resolves to a loopback address: 127.0.1.1; using 192.168.1.11 instead (on interface wlp3s0)\n",
      "22/07/22 01:27:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/07/22 01:27:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"SparkByExamples.com\") \\\n",
    "      .getOrCreate() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NA Thresholding for Columns and Rows\n",
    "\n",
    "# python\n",
    "def null_threshold(data, limit, ax):\n",
    "    '''\n",
    "    input: data -> pandas dataframe\n",
    "            limit -> float [0.0(no null values allowed), 1.0(all null values allowed)] \n",
    "            ax -> int (0 for row based thresholding, 1 for column based)\n",
    "    Output: pd.DataFrame\n",
    "    '''\n",
    "    if ax == 1:\n",
    "        return (data.dropna(axis=ax, thresh=int(limit * len(data))))\n",
    "    else :\n",
    "        return (data.dropna(axis=ax, thresh=int(limit * len(data.columns))))\n",
    "\n",
    "\n",
    "# pyspark\n",
    "# add if else for axis\n",
    "from pyspark.sql.functions import count, when, col\n",
    "def null_threshold_pyspark(data, limit, ax):\n",
    "    '''\n",
    "    input: data -> spark dataframe\n",
    "            limit -> float [0.0(no null values allowed), 1.0(all null values allowed)] \n",
    "            ax -> int (0 for row based thresholding, 1 for column based)\n",
    "    Output: Spark DataFrame\n",
    "    '''\n",
    "    if ax == 1:\n",
    "        aggregated_row = data.select([(count(when(col(c).isNull(), c))/data.count()).alias(c) for c in data.columns]).collect()\n",
    "        aggregated_dict_list = [row.asDict() for row in aggregated_row]\n",
    "        aggregated_dict = aggregated_dict_list[0] \n",
    "        col_null_g_limit_p=list({i for i in aggregated_dict if aggregated_dict[i] >= (1-limit)})\n",
    "        temp = data.drop(*col_null_g_limit_p)\n",
    "        return (temp)\n",
    "    else :\n",
    "        return (data.dropna(thresh= int(limit * len(data.columns))))\n",
    "\n",
    "\n",
    "# corner case: print(null_threshold(df, 0.7, 1)) null_threshold_pyspark(df_ps, 0.7, 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows if missing values are present in specific columns\n",
    "\n",
    "# python\n",
    "def null_unique_columns(data, unique_cols):\n",
    "    '''\n",
    "    input: data -> pd.DataFrame\n",
    "            unique_cols -> list of column(s) that should be unique\n",
    "    output: pd.DataFrame\n",
    "    '''\n",
    "    return (data.dropna(subset=unique_cols))\n",
    "\n",
    "#pyspark\n",
    "from functools import reduce\n",
    "def null_unique_columns_pyspark(data, unique_cols):\n",
    "    '''\n",
    "    input: data -> Spark DataFrame\n",
    "            unique_cols -> list of column(s) that should be unique\n",
    "    output: Spark DataFrame\n",
    "    '''\n",
    "    return (data.na.drop(subset=unique_cols))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation\n",
    "1. Statistical Imputation <br>\n",
    "<t> 1.1 Mean, mode, median strategy\n",
    "<t> 1.2 Constant Strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NA\n",
    "\n",
    "# python\n",
    "def fill_na(data):\n",
    "    '''\n",
    "    Description: Fills na of quantative columns with median and qualitative columns with mode\n",
    "    input: data -> pd.DataFrame\n",
    "    output: pd.DataFrame\n",
    "    '''\n",
    "    # looking for best method for each column\n",
    "    for i in data.columns:\n",
    "        if (data[i].dtype in ['object', 'bool']):\n",
    "            # mode by default doesn't consider NaN for mode()\n",
    "            data[i].fillna(data[i].mode()[0], inplace=True)\n",
    "        else:\n",
    "            data[i].fillna(data[i].median()[0], inplace=True)\n",
    "    return data\n",
    "\n",
    "# pyspark\n",
    "def fill_na_pyspark(data):\n",
    "    '''\n",
    "    Description: Fills na of quantative columns with median and qualitative columns with mode\n",
    "    input: data -> spark dataframe\n",
    "    output: spark dataframe\n",
    "    '''\n",
    "    for i in data.dtypes:\n",
    "        if (data.i[1] in ['string']):\n",
    "            # calculating mode\n",
    "            mode_temp = data.groupBy(i[0]).count().orderBy(\"count\", ascending=False).first()[0]\n",
    "            return (data.na.fill(value=mode_temp, subset=[i[0]]))\n",
    "        elif (data.i[1] == 'int'):\n",
    "            # calculating median\n",
    "            median_temp = data.approxQuantile(i[0], [0.5], 0.25)\n",
    "            return (data.na.fill(value=median_temp, subset=[i[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible alternative\n",
    "\n",
    "# python\n",
    "from os import XATTR_REPLACE\n",
    "from numpy import isnan\n",
    "from sklearn.impute import SimpleImputer\n",
    "def fill_na_imp (X, method):\n",
    "    ''' \n",
    "    Input: X (pd.DataFrame), method = ['mean', 'median', 'most_frequent', 'constant']\n",
    "    '''\n",
    "    imputer = SimpleImputer(strategy=method)\n",
    "    imputer.fit(X)\n",
    "    return (imputer.transform(X))\n",
    "\n",
    "\n",
    "# pyspark\n",
    "\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols = ['Age of Employee', 'Experience (in years)', 'Salary (per month - $)'],\n",
    "    outputCols = [\"{}_imputed\".format(a) for a in ['Age of Employee', 'Experience (in years)', 'Salary (per month - $)']]\n",
    ").setStrategy(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNNImputer\n",
    "\n",
    "# python\n",
    "\n",
    "# ques: more methods\n",
    "from sklearn.impute import KNNImputer\n",
    "def KNN_imputer(X, k, method):\n",
    "    ''' \n",
    "    Input: np matrix, make sure all null values marked as NaN\n",
    "    method = ['nan_euclidean']\n",
    "\n",
    "    '''\n",
    "    imputer = KNNImputer(n_neighbors=k, weights='uniform', metric=method)\n",
    "    imputer.fit(X)\n",
    "    Xtrans = imputer.transform(X)\n",
    "    return (Xtrans)\n",
    "\n",
    "# pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('test.csv')\n",
    "df_ps = spark.read.csv('test.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance thresholding\n",
    "\n",
    "# Ques. range of variance limit in both cases\n",
    "\n",
    "# python\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "def variance_thresholding(data, limit):\n",
    "    '''\n",
    "    Input: pd.DataFrame, limit 0-> all features, 1 -> no features\n",
    "    Ouput: X (numpy matrix), y (numpy matrix)\n",
    "    '''\n",
    "    df = data.values\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "    transform = VarianceThreshold(threshold=limit)\n",
    "    X_sel = transform.fit_transform(X)\n",
    "    return (X_sel, y)\n",
    "\n",
    "# pyspark (check code)\n",
    "from pyspark.ml.feature import VarianceThresholdSelector\n",
    "def variance_thresholding_pyspark(data, limit):\n",
    "    '''\n",
    "    Input: Spark Dataframe, limit (int) \n",
    "    Ouput: \n",
    "    '''\n",
    "    selector = VarianceThresholdSelector(varianceThreshold=limit, outputCol='selected_features')\n",
    "    model = selector.fit(data)\n",
    "    return (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate Data\n",
    "# questions: subsets?\n",
    "\n",
    "# python\n",
    "def drop_duplicates(data):\n",
    "    '''\n",
    "    Input: pd.DataFrame\n",
    "    output: pd.DataFrame\n",
    "    '''\n",
    "    return (data.drop_duplicates)\n",
    "\n",
    "# pyspark\n",
    "def drop_duplicates_pyspark(data):\n",
    "    '''\n",
    "    '''\n",
    "    return ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Column Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to numerical\n",
    "\n",
    "# python\n",
    "\n",
    "# pyspark\n",
    "from pyspark.sql import functions as f \n",
    "from pyspark.sql.types import IntegerType\n",
    "def to_numerical_pyspark (data, cols):\n",
    "    ''' \n",
    "    '''\n",
    "    for column in cols:\n",
    "        data = data.withColumn(column, f.col(column).cast(IntegerType))\n",
    "    return (data)\n",
    "\n",
    "# list of numerical columns\n",
    "\n",
    "# python\n",
    "\n",
    "# pyspark\n",
    "def numerical_cols_pyspark (data):\n",
    "    numeric_cols = [column[0] for column in df.dtypes if column[1] == 'int']\n",
    "    return (numeric_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# to datetime\n",
    "\n",
    "# encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "# python\n",
    "def outlier_threshold (data, method, limit):\n",
    "    ''' \n",
    "    Input: pd.DataFrame, method = standard_deviation, inter_quartile, limit\n",
    "    '''\n",
    "    return ()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark\n",
    "\n",
    "def find_outliers_pyspark(df):\n",
    "    ''' \n",
    "    Input: Spark DF\n",
    "    OutputL Spark DF with a new column with number of outliers 'total_outliers' in the record\n",
    "    '''\n",
    "    # Identifying the numerical columns in a spark dataframe\n",
    "    numeric_columns = numerical_cols_pyspark(df)\n",
    "\n",
    "    # Using the `for` loop to create new columns by identifying the outliers for each feature\n",
    "    for column in numeric_columns:\n",
    "\n",
    "        less_Q1 = 'less_Q1_{}'.format(column)\n",
    "        more_Q3 = 'more_Q3_{}'.format(column)\n",
    "        Q1 = 'Q1_{}'.format(column)\n",
    "        Q3 = 'Q3_{}'.format(column)\n",
    "\n",
    "        # Q1 : First Quartile ., Q3 : Third Quartile\n",
    "        Q1 = df.approxQuantile(column,[0.25],relativeError=0)\n",
    "        Q3 = df.approxQuantile(column,[0.75],relativeError=0)\n",
    "        \n",
    "        # IQR : Inter Quantile Range\n",
    "        # We need to define the index [0], as Q1 & Q3 are a set of lists., to perform a mathematical operation\n",
    "        # Q1 & Q3 are defined seperately so as to have a clear indication on First Quantile & 3rd Quantile\n",
    "        IQR = Q3[0] - Q1[0]\n",
    "        \n",
    "        #selecting the data, with -1.5*IQR to + 1.5*IQR., where param = 1.5 default value\n",
    "        less_Q1 =  Q1[0] - 1.5*IQR\n",
    "        more_Q3 =  Q3[0] + 1.5*IQR\n",
    "        \n",
    "        isOutlierCol = 'is_outlier_{}'.format(column)\n",
    "        \n",
    "        df = df.withColumn(isOutlierCol,f.when((df[column] > more_Q3) | (df[column] < less_Q1), 1).otherwise(0))\n",
    "    \n",
    "\n",
    "    # Selecting the specific columns which we have added above, to check if there are any outliers\n",
    "    selected_columns = [column for column in df.columns if column.startswith(\"is_outlier\")]\n",
    "\n",
    "    # Adding all the outlier columns into a new colum \"total_outliers\", to see the total number of outliers\n",
    "    df = df.withColumn('total_outliers',sum(df[column] for column in selected_columns))\n",
    "\n",
    "    # Dropping the extra columns created above, just to create nice dataframe., without extra columns\n",
    "    df = df.drop(*[column for column in df.columns if column.startswith(\"is_outlier\")])\n",
    "\n",
    "    return df\n",
    "\n",
    "def outlier_threshold_pyspark (df, limit):\n",
    "    ''' \n",
    "    '''\n",
    "    df = find_outliers_pyspark(df)\n",
    "    len_numeric = len(numerical_cols_pyspark(df))\n",
    "    return (df.filter(df['total_outliers'] < limit * len_numeric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatic outlier treatment\n",
    "\n",
    "# python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "def automatic_outlier (X_train, y_train):\n",
    "    ''' \n",
    "    Input: numpy matrix of training set\n",
    "    '''\n",
    "    lof = LocalOutlierFactor()\n",
    "    yhat = lof.fit_predict(X_train)\n",
    "    mask = yhat != -1\n",
    "    X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "    return (X_train, y_train)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('def')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64a09c849e20cd04ef7da81fed90aeaa7ded775c411421857a34c4c7bb13fbe0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
